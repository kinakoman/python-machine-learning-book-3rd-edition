# 決定木学習

決定木モデルでは訓練データセットの特徴量に基づいて一連の質問を学習し、データ点のクラスラベルを推定する。

決定木の根(ルート)から始めて情報利得(information gain)が最大となる特徴量でデータを分割する。
情報利得が最大とは、分割後の要素においてクラスのばらつきがないことを指す。

質問の分岐点はノードと呼ばれ、ノードの先(データが分割されている点)をリーフ(葉ノード)と呼ぶ。
決定木のアルゴリズムではリーフが純粋(リーフ内の分割されたデータのばらつきが少ない状態)
までのノードによる分割を繰り返す。

優れたノード(質問)は、情報利得の高い、すなわち分割後のデータのばらつきがより小さいものであると言える。
決定木アルゴリズムでは情報利得の高くなる特徴量を探し、これをノードとしてデータを分割する。

決定木アルゴリズムにおける最適化の目的関数は以下のように示される。

$$
IG(D_p,f)=I(D_p)-\sum_{j=1}^m\dfrac{N_j}{N_p}I(D_j)
$$

ここで、 $f$ は分割を行う特徴量、 $D_p$ は分割前の親のデータセット、 $D_j$ はj番目の子ノードのデータセットを示す。
すなわち、親ノードが $m$ 個の子ノードに分割されている状態を想定している。
$I$ は不純度を数値化したものを示す。 $N_p$ は親ノードのデータ点の総数、 $N_j$ はj番目の子
データ点の個数である。式に示される通り、
情報利得は親ノードの不純度と子ノードの不純度の差で定義される。すなわち、分割後の子ノードのばらつきが小さいほど
情報利得は大きくなる。

話を単運化して、 $m個の$